# Usage Guide: Dataflow-Aware Security Scanner

## Quick Start

The scanner now automatically uses AST-based program slicing for Python files. No configuration changes needed!

```bash
# 1. Ensure dependencies are installed
pip install -r requirements.txt

# 2. Configure your .env file (if not already done)
# Add your REPO, GITHUB_TOKEN, and OPENAI_API_KEY

# 3. Run the scanner
python scan.py
```

## What's New?

### Automatic Program Slicing

When analyzing vulnerabilities, the scanner now:

1. **Parses** the source code into an AST
2. **Builds** a dataflow-aware program slice showing:
   - The vulnerable function (sink)
   - How data flows to the vulnerability (upstream dataflow)
   - Validation/sanitization functions (helpers)
   - Functions that call the vulnerable code (callers)
3. **Sends** this structured slice to the AI for analysis

### Benefits

- **Fewer False Positives**: AI sees if data is validated before reaching the sink
- **Better Context**: AI understands the complete dataflow, not just nearby lines
- **Targeted Analysis**: Only relevant code is sent, reducing noise and cost

## Example Output

### Before (Line-Based Context)

```
AI receives:
- Lines 1-100 (file header)
- Lines 450-550 (¬±50 around vulnerability)

Problem: May miss validation logic at line 200!
```

### After (Program Slicing)

```
AI receives:
--- SINK_CONTEXT ---
def execute_command(cmd):
    result = subprocess.run(cmd, shell=True)  # ‚Üê Vulnerable line
    return result

--- UPSTREAM_DATAFLOW ---
Line 20: cmd = request.args.get('command')  # ‚Üê Tainted source
Line 21: cmd = validate_input(cmd)          # ‚Üê Validation!

--- HELPERS_AND_SANITIZERS ---
def validate_input(user_input):             # ‚Üê AI sees this function!
    if user_input not in ALLOWED_COMMANDS:
        raise ValueError("Invalid command")
    return user_input

Result: AI correctly identifies this as SAFE (false positive)
```

## Viewing Results

### Console Output

During analysis, you'll see:

```
[1/16] Analyzing: temp_repo\app.py - command-injection
  ‚úì TRUE POSITIVE - Risk: CRITICAL
  
[2/16] Analyzing: temp_repo\utils.py - sql-injection  
  ‚úó FALSE POSITIVE
```

### JSON Output (`ai_analysis.json`)

Each finding now includes slice metadata:

```json
{
  "semgrep_finding": {
    "check_id": "python.flask.security.command-injection",
    "file": "app.py",
    "line": 23
  },
  "ai_analysis": {
    "is_vulnerable": false,
    "confidence": "high",
    "risk_level": "INFO",
    "reasoning": "The 'command' variable flows through validate_input() 
                  which implements a whitelist check against ALLOWED_COMMANDS. 
                  The validation function (lines 10-14) ensures only safe commands
                  are executed. This is a false positive.",
    "recommendation": "No fix needed. The existing validation is sufficient.",
    "slice_metadata": {
      "sink_line": 23,
      "sink_function": "execute_command",
      "suspicious_vars": ["command"],
      "used_fallback": false,
      "total_slice_lines": 45
    }
  }
}
```

## Advanced Usage

### Testing the Slicer

Run the included test:

```bash
python test_slicer.py
```

Expected output:
```
üî¨ Testing Program Slicer

TEST 1: Vulnerable Command Execution (Line 23)
‚úÖ Program slice generated successfully!
‚úÖ Detected vulnerable dataflow

TEST 2: Safe Command Execution with Validation (Line 35)
‚úÖ Program slice generated successfully!
‚úÖ Validation function detected in helpers!

üéâ All tests passed!
```

### Fallback Behavior

The slicer automatically falls back to line-based context if:

- The file has syntax errors
- The language is not Python (for now)
- AST parsing fails for any reason

You'll see this in the output:

```
[3/16] Analyzing: temp_repo\file.js - xss-vulnerability
  Warning: Program slicing failed, using fallback for file.js:42
  ‚úì TRUE POSITIVE - Risk: HIGH
```

The analysis continues normally, but uses the old method.

### Debugging

Enable verbose output by checking the console:

```python
# In scan.py, the analyze_vulnerability_with_ai function logs:
print(f"  Debug: File not found at: {full_path}")
print(f"  Warning: Program slicing failed: {e}")
print(f"  Info: AST slicing not yet supported for {language}, using fallback")
```

## Understanding the Slice Sections

### 1. SINK_CONTEXT

**What it is**: The complete function containing the vulnerability

**Why it matters**: Shows the sink in its natural context

**Example**:
```python
def search_user(username):
    query = f"SELECT * FROM users WHERE name = '{username}'"  # ‚Üê Sink
    return db.execute(query)
```

### 2. UPSTREAM_DATAFLOW

**What it is**: Variable definitions and transformations leading to the sink

**Why it matters**: Shows where data comes from and how it's modified

**Example**:
```python
Line 10: username = request.args.get('user')  # ‚Üê Source
Line 11: username = username.strip()           # ‚Üê Transformation
Line 12: username = username.lower()           # ‚Üê Another transformation
```

### 3. HELPERS_AND_SANITIZERS

**What it is**: Functions that process the suspicious data

**Why it matters**: Critical for identifying false positives

**Example**:
```python
def sanitize_sql_input(text):
    return text.replace("'", "''").replace(";", "")

# If the upstream dataflow shows:
# username = sanitize_sql_input(username)
# Then the AI knows it's sanitized!
```

### 4. CALLERS_AND_ENTRYPOINTS

**What it is**: Functions that call the vulnerable function (limited to 2)

**Why it matters**: Shows how untrusted data enters the vulnerable function

**Example**:
```python
@app.route('/api/search')
def api_search():
    user_input = request.form.get('query')  # ‚Üê Untrusted source
    return search_user(user_input)          # ‚Üê Passes to vulnerable function
```

## Customization

### Adjusting Slice Size

In `scan.py`, modify the `max_lines` parameter:

```python
program_slice = build_program_slice(
    file_content=full_file_content,
    file_path=file_path,
    sink_line=start_line,
    semgrep_dataflow=dataflow_trace,
    max_lines=600  # ‚Üê Increase for more context, decrease for cost savings
)
```

### Adding More Languages

To add support for another language:

1. Create a new slicer class in `program_slicer.py`:

```python
class JavaScriptSlicer:
    def __init__(self, source_code: str, file_path: str):
        # Use a JS parser like esprima, acorn, or babel
        self.tree = parse_javascript(source_code)
    
    def build_slice(self, sink_line, suspicious_vars, max_lines):
        # Implement JS-specific slicing logic
        ...
```

2. Update `build_program_slice()`:

```python
if language == 'python':
    slicer = PythonSlicer(file_content, file_path)
elif language == 'javascript':
    slicer = JavaScriptSlicer(file_content, file_path)
else:
    # Fallback
    ...
```

## Performance

### Benchmarks

Typical performance on a ~500 line Python file:

- **AST Parsing**: ~30-50ms
- **Program Slicing**: ~80-120ms  
- **Total Overhead**: ~100-170ms per finding
- **OpenAI API Call**: 2000-5000ms

**Conclusion**: Slicing adds minimal overhead (<5% of total analysis time)

### Optimization Tips

1. **Reduce max_callers** if you have large codebases:
   ```python
   # In program_slicer.py
   callers_and_entrypoints = self._extract_callers(
       sink_function.name, max_callers=1  # ‚Üê Reduce from 2 to 1
   )
   ```

2. **Skip slicing for small files** (optional):
   ```python
   # In scan.py
   if len(full_file_content) < 5000:  # Small file, send everything
       return analyze_vulnerability_with_ai_fallback(vulnerability, full_file_content)
   ```

## Troubleshooting

### Issue: "Syntax error parsing file.py"

**Cause**: The Python file has invalid syntax

**Solution**: 
- Fix the syntax error in the source file, or
- The scanner will automatically use fallback mode

### Issue: "No definitions found for variables: ['x', 'y']"

**Cause**: Variables are defined outside the current function (e.g., global scope, imported)

**Impact**: Minimal - the AI still sees the sink context

**Solution**: Future enhancement could add interprocedural analysis

### Issue: Slice seems incomplete

**Cause**: Variable tracking is limited to 3 levels of recursion (to prevent infinite loops)

**Solution**: Increase `max_iterations` in `_extract_upstream_dataflow()`:

```python
# In program_slicer.py, PythonSlicer class
max_iterations = 5  # ‚Üê Increase from 3 to 5
```

## Best Practices

1. **Review AI reasoning**: The AI now references specific functions and dataflow - verify these references
2. **Check slice_metadata**: If `used_fallback: true`, the analysis used line-based context
3. **Update your .env**: Use `gpt-4.1-mini` for best results with program slicing
4. **Keep files modular**: Slicing works best when sanitization logic is in separate functions

## Next Steps

- Run the scanner on your repository
- Review the `ai_analysis.json` output
- Check the `slice_metadata` to see how many findings used slicing vs fallback
- Report any issues or suggestions for improvement

Happy scanning! üîçüõ°Ô∏è

